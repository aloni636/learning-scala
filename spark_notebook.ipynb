{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7912ddcd-c660-4c41-b135-9dcfa3bd148f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                  \u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:3.3.4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ba8a082-8645-45c1-b08c-280f2bfc3324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SparkSession\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07076e46-df44-4dd7-b897-3abe3073ccd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres2_0\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"3.3.4\"\u001b[39m\n",
       "\u001b[36mres2_1\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"2.13.8\"\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org.apache.spark.SPARK_VERSION\n",
    "scala.util.Properties.versionNumberString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "472541ea-5d51-441d-a4de-799659265eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/12/20 13:00:30 INFO SparkContext: Running Spark version 3.3.4\n",
      "25/12/20 13:00:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/12/20 13:00:30 INFO ResourceUtils: ==============================================================\n",
      "25/12/20 13:00:30 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "25/12/20 13:00:30 INFO ResourceUtils: ==============================================================\n",
      "25/12/20 13:00:30 INFO SparkContext: Submitted application: helloJupyter\n",
      "25/12/20 13:00:30 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "25/12/20 13:00:30 INFO ResourceProfile: Limiting resource is cpu\n",
      "25/12/20 13:00:30 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "25/12/20 13:00:30 INFO SecurityManager: Changing view acls to: ubuntu\n",
      "25/12/20 13:00:30 INFO SecurityManager: Changing modify acls to: ubuntu\n",
      "25/12/20 13:00:30 INFO SecurityManager: Changing view acls groups to: \n",
      "25/12/20 13:00:30 INFO SecurityManager: Changing modify acls groups to: \n",
      "25/12/20 13:00:30 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ubuntu); groups with view permissions: Set(); users  with modify permissions: Set(ubuntu); groups with modify permissions: Set()\n",
      "25/12/20 13:00:30 INFO Utils: Successfully started service 'sparkDriver' on port 35821.\n",
      "25/12/20 13:00:30 INFO SparkEnv: Registering MapOutputTracker\n",
      "25/12/20 13:00:30 INFO SparkEnv: Registering BlockManagerMaster\n",
      "25/12/20 13:00:30 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "25/12/20 13:00:30 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "25/12/20 13:00:30 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "25/12/20 13:00:30 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-e850f02b-598b-48a0-8398-ead6b1e1bca4\n",
      "25/12/20 13:00:31 INFO MemoryStore: MemoryStore started with capacity 2.1 GiB\n",
      "25/12/20 13:00:31 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "25/12/20 13:00:31 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "25/12/20 13:00:31 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-localhost:7077...\n",
      "25/12/20 13:00:31 INFO TransportClientFactory: Successfully created connection to spark-localhost/172.17.0.2:7077 after 22 ms (0 ms spent in bootstraps)\n",
      "25/12/20 13:00:31 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20251220130031-0001\n",
      "25/12/20 13:00:31 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35607.\n",
      "25/12/20 13:00:31 INFO NettyBlockTransferService: Server created on spark-localhost:35607\n",
      "25/12/20 13:00:31 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20251220130031-0001/0 on worker-20251220111919-172.17.0.2-36179 (172.17.0.2:36179) with 12 core(s)\n",
      "25/12/20 13:00:31 INFO StandaloneSchedulerBackend: Granted executor ID app-20251220130031-0001/0 on hostPort 172.17.0.2:36179 with 12 core(s), 1024.0 MiB RAM\n",
      "25/12/20 13:00:31 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "25/12/20 13:00:31 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, spark-localhost, 35607, None)\n",
      "25/12/20 13:00:31 INFO BlockManagerMasterEndpoint: Registering block manager spark-localhost:35607 with 2.1 GiB RAM, BlockManagerId(driver, spark-localhost, 35607, None)\n",
      "25/12/20 13:00:31 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, spark-localhost, 35607, None)\n",
      "25/12/20 13:00:31 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, spark-localhost, 35607, None)\n",
      "25/12/20 13:00:31 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20251220130031-0001/0 is now RUNNING\n",
      "25/12/20 13:00:31 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@6b814250"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession.builder()\n",
    "    .appName(\"helloJupyter\")\n",
    "    .master(s\"spark://spark-localhost:7077\")\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6777b07-0667-42c7-96e5-f10867d2f9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/12/20 13:00:44 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "25/12/20 13:00:44 INFO SharedState: Warehouse path is 'file:/workspaces/learning-scala/spark-warehouse'.\n",
      "25/12/20 13:00:45 INFO CodeGenerator: Code generated in 148.924052 ms\n",
      "25/12/20 13:00:45 INFO CodeGenerator: Code generated in 6.877717 ms\n",
      "25/12/20 13:00:45 INFO CodeGenerator: Code generated in 9.137075 ms\n",
      "+----------+---+\n",
      "|first_name|age|\n",
      "+----------+---+\n",
      "|       sue| 32|\n",
      "|        li|  3|\n",
      "|       bob| 75|\n",
      "|       heo| 13|\n",
      "+----------+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mdata\u001b[39m: \u001b[32mSeq\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mInt\u001b[39m)] = \u001b[33mList\u001b[39m(\n",
       "  (\u001b[32m\"sue\"\u001b[39m, \u001b[32m32\u001b[39m),\n",
       "  (\u001b[32m\"li\"\u001b[39m, \u001b[32m3\u001b[39m),\n",
       "  (\u001b[32m\"bob\"\u001b[39m, \u001b[32m75\u001b[39m),\n",
       "  (\u001b[32m\"heo\"\u001b[39m, \u001b[32m13\u001b[39m)\n",
       ")\n",
       "\u001b[36mdf\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [first_name: string, age: int]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "\n",
    "val data = Seq((\"sue\", 32), (\"li\", 3), (\"bob\", 75), (\"heo\", 13))\n",
    "val df = data.toDF(\"first_name\", \"age\")\n",
    "\n",
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
