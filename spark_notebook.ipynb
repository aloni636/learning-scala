{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c43da3a-a986-4163-bc62-0d3ef9a5f50f",
   "metadata": {},
   "source": [
    "# Packages & Spark Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7912ddcd-c660-4c41-b135-9dcfa3bd148f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                  \u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:3.5.5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07076e46-df44-4dd7-b897-3abe3073ccd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres1_0\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"3.5.7\"\u001b[39m\n",
       "\u001b[36mres1_1\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"2.13.8\"\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org.apache.spark.SPARK_VERSION\n",
    "scala.util.Properties.versionNumberString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "899e2116-c0fe-4cea-bef4-1a8b1f945dfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.NotebookSparkSession\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SparkSession\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.NotebookSparkSession\n",
    "import org.apache.spark.sql.SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "472541ea-5d51-441d-a4de-799659265eb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "26/01/04 20:17:19 INFO SparkContext: Running Spark version 3.5.7\n",
      "26/01/04 20:17:19 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64\n",
      "26/01/04 20:17:19 INFO SparkContext: Java version 17.0.17\n",
      "26/01/04 20:17:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "26/01/04 20:17:20 INFO ResourceUtils: ==============================================================\n",
      "26/01/04 20:17:20 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "26/01/04 20:17:20 INFO ResourceUtils: ==============================================================\n",
      "26/01/04 20:17:20 INFO SparkContext: Submitted application: helloJupyter\n",
      "26/01/04 20:17:20 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 4096, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "26/01/04 20:17:20 INFO ResourceProfile: Limiting resource is cpu\n",
      "26/01/04 20:17:20 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "26/01/04 20:17:20 INFO SecurityManager: Changing view acls to: ubuntu\n",
      "26/01/04 20:17:20 INFO SecurityManager: Changing modify acls to: ubuntu\n",
      "26/01/04 20:17:20 INFO SecurityManager: Changing view acls groups to: \n",
      "26/01/04 20:17:20 INFO SecurityManager: Changing modify acls groups to: \n",
      "26/01/04 20:17:20 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: ubuntu; groups with view permissions: EMPTY; users with modify permissions: ubuntu; groups with modify permissions: EMPTY\n",
      "26/01/04 20:17:20 INFO Utils: Successfully started service 'sparkDriver' on port 34041.\n",
      "26/01/04 20:17:20 INFO SparkEnv: Registering MapOutputTracker\n",
      "26/01/04 20:17:20 INFO SparkEnv: Registering BlockManagerMaster\n",
      "26/01/04 20:17:20 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "26/01/04 20:17:20 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "26/01/04 20:17:20 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "26/01/04 20:17:20 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-17ce1929-19ed-4d92-a5b2-43bd8344a882\n",
      "26/01/04 20:17:20 INFO MemoryStore: MemoryStore started with capacity 2.1 GiB\n",
      "26/01/04 20:17:20 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "26/01/04 20:17:20 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "26/01/04 20:17:20 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "26/01/04 20:17:20 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077...\n",
      "26/01/04 20:17:21 INFO TransportClientFactory: Successfully created connection to localhost/127.0.0.1:7077 after 34 ms (0 ms spent in bootstraps)\n",
      "26/01/04 20:17:21 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20260104201721-0000\n",
      "26/01/04 20:17:21 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43831.\n",
      "26/01/04 20:17:21 INFO NettyBlockTransferService: Server created on learning-scala:43831\n",
      "26/01/04 20:17:21 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "26/01/04 20:17:21 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, learning-scala, 43831, None)\n",
      "26/01/04 20:17:21 INFO BlockManagerMasterEndpoint: Registering block manager learning-scala:43831 with 2.1 GiB RAM, BlockManagerId(driver, learning-scala, 43831, None)\n",
      "26/01/04 20:17:21 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, learning-scala, 43831, None)\n",
      "26/01/04 20:17:21 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, learning-scala, 43831, None)\n",
      "26/01/04 20:17:21 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20260104201721-0000/0 on worker-20260104201637-localhost-32853 (localhost:32853) with 4 core(s)\n",
      "26/01/04 20:17:21 INFO StandaloneSchedulerBackend: Granted executor ID app-20260104201721-0000/0 on hostPort localhost:32853 with 4 core(s), 4.0 GiB RAM\n",
      "26/01/04 20:17:21 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20260104201721-0000/1 on worker-20260104201635-localhost-45543 (localhost:45543) with 4 core(s)\n",
      "26/01/04 20:17:21 INFO StandaloneSchedulerBackend: Granted executor ID app-20260104201721-0000/1 on hostPort localhost:45543 with 4 core(s), 4.0 GiB RAM\n",
      "26/01/04 20:17:21 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20260104201721-0000/2 on worker-20260104201639-localhost-40251 (localhost:40251) with 4 core(s)\n",
      "26/01/04 20:17:21 INFO StandaloneSchedulerBackend: Granted executor ID app-20260104201721-0000/2 on hostPort localhost:40251 with 4 core(s), 4.0 GiB RAM\n",
      "26/01/04 20:17:21 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20260104201721-0000/0 is now RUNNING\n",
      "26/01/04 20:17:21 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20260104201721-0000/1 is now RUNNING\n",
      "26/01/04 20:17:21 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20260104201721-0000/2 is now RUNNING\n",
      "26/01/04 20:17:21 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@6628f921\n",
       "\u001b[36msc\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mSparkContext\u001b[39m = org.apache.spark.SparkContext@6ba1f583\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession.builder()\n",
    "    .appName(\"helloJupyter\")\n",
    "    .master(s\"spark://localhost:7077\")\n",
    "    .config(\"spark.executor.memory\", \"4g\")\n",
    "    .getOrCreate()\n",
    "val sc = spark.sparkContext\n",
    "\n",
    "sc.setLogLevel(\"WARN\")\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "768bfbdc-57ec-4664-936a-09c723253fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel.silent(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d68faea-671f-448b-a158-07a70fa1bd73",
   "metadata": {},
   "source": [
    "# Interactive Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6777b07-0667-42c7-96e5-f10867d2f9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "// val data = Seq((\"sue\", 32), (\"li\", 3), (\"bob\", 75), (\"heo\", 13))\n",
    "// val df = data.toDF(\"first_name\", \"age\")\n",
    "\n",
    "// df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4596018b-8899-46e9-ade3-4429b147b399",
   "metadata": {},
   "source": [
    "# Task 1: Trip Flow Efficiency\n",
    "\n",
    "**Goal:** Rank the top 20 busiest **Pickup  Drop-off** pairs during weekday rush hours.\n",
    "\n",
    "* **Filter:** Weekdays + Rush Hour windows.\n",
    "* **Key:** `(PULocationID, DOLocationID)`.\n",
    "* **Metrics:** Count, Avg Duration, Avg Speed, Avg Fare/Mile.\n",
    "* **RDD Ops:** `filter`  `map` (to composite key)  `reduceByKey` (summing metrics)  `mapValues` (calculating averages)  `takeOrdered`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91e98d99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.{functions => F, types => T}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.Logger\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.{SparkSession, DataFrame}\u001b[39m"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.{functions => F, types => T}\n",
    "import org.apache.log4j.Logger\n",
    "import org.apache.spark.sql.{SparkSession, DataFrame}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04c85a8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mobject\u001b[39m \u001b[36mSchemaEnforcer\u001b[39m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object SchemaEnforcer {\n",
    "    // format: off\n",
    "    val schema = T.StructType(\n",
    "      Seq(\n",
    "        T.StructField(\"VendorID\", T.IntegerType, nullable = true),\n",
    "        T.StructField(\"tpep_pickup_datetime\", T.TimestampType, nullable = true),\n",
    "        T.StructField(\"tpep_dropoff_datetime\", T.TimestampType, nullable = true),\n",
    "        T.StructField(\"passenger_count\", T.DoubleType, nullable = true),\n",
    "        T.StructField(\"trip_distance\", T.DoubleType, nullable = true),\n",
    "        T.StructField(\"RatecodeID\", T.LongType, nullable = true),\n",
    "        T.StructField(\"store_and_fwd_flag\", T.StringType, nullable = true),\n",
    "        T.StructField(\"PULocationID\", T.IntegerType, nullable = true),\n",
    "        T.StructField(\"DOLocationID\", T.IntegerType, nullable = true),\n",
    "        T.StructField(\"payment_type\", T.LongType, nullable = true),\n",
    "        T.StructField(\"fare_amount\", T.DoubleType, nullable = true),\n",
    "        T.StructField(\"extra\", T.DoubleType, nullable = true),\n",
    "        T.StructField(\"mta_tax\", T.DoubleType, nullable = true),\n",
    "        T.StructField(\"tip_amount\", T.DoubleType, nullable = true),\n",
    "        T.StructField(\"tolls_amount\", T.DoubleType, nullable = true),\n",
    "        T.StructField(\"improvement_surcharge\", T.DoubleType, nullable = true),\n",
    "        T.StructField(\"total_amount\", T.DoubleType, nullable = true),\n",
    "        T.StructField(\"congestion_surcharge\", T.DoubleType, nullable = true),\n",
    "        T.StructField(\"airport_fee\", T.DoubleType, nullable = true)\n",
    "      )\n",
    "    )\n",
    "    // format: on\n",
    "    private def normName(name: String): String = {\n",
    "      name.toLowerCase().replaceAll(\"_|-\", \"\")\n",
    "    }\n",
    "    private val normNameTypePair =\n",
    "      this.schema.map(f => (normName(f.name), f.name, f.dataType))\n",
    "\n",
    "    def enforce(df: DataFrame)(implicit log: Logger): DataFrame = {\n",
    "      val dfNormNameToRawPair =\n",
    "        df.schema.map(f => this.normName(f.name) -> (f.name, f.dataType)).toMap\n",
    "\n",
    "      val newCols = normNameTypePair.map { case (norm, name, dataType) =>\n",
    "        val (ogName, ogDataType) = dfNormNameToRawPair(norm)\n",
    "        var newCol = F.col(ogName)\n",
    "        if (name != ogName) {\n",
    "          log.info(s\"Renaming '$ogName' to '$name'\")\n",
    "          newCol = newCol.alias(name)\n",
    "        }\n",
    "        if (dataType != ogDataType) {\n",
    "          log.warn(s\"Casting '$name' from $ogDataType to $dataType\")\n",
    "          newCol = newCol.cast(dataType)\n",
    "        }\n",
    "        newCol\n",
    "      }\n",
    "      df.select(newCols: _*)\n",
    "    }\n",
    "  }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "260b260c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.hadoop.fs.{FileSystem, Path}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.hadoop.conf.Configuration\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mconf\u001b[39m: \u001b[32mConfiguration\u001b[39m = Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-rbf-default.xml, hdfs-site.xml, hdfs-rbf-site.xml\n",
       "\u001b[36mfs\u001b[39m: \u001b[32mFileSystem\u001b[39m = org.apache.hadoop.fs.LocalFileSystem@3a3751ed\n",
       "\u001b[36mparquetPaths\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mArray\u001b[39m(\n",
       "  \u001b[32m\"file:/workspaces/learning-scala/data/taxi/yellow_tripdata_2023-01.parquet\"\u001b[39m,\n",
       "  \u001b[32m\"file:/workspaces/learning-scala/data/taxi/yellow_tripdata_2023-02.parquet\"\u001b[39m,\n",
       "  \u001b[32m\"file:/workspaces/learning-scala/data/taxi/yellow_tripdata_2023-03.parquet\"\u001b[39m,\n",
       "  \u001b[32m\"file:/workspaces/learning-scala/data/taxi/yellow_tripdata_2023-04.parquet\"\u001b[39m,\n",
       "  \u001b[32m\"file:/workspaces/learning-scala/data/taxi/yellow_tripdata_2023-05.parquet\"\u001b[39m\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.hadoop.fs.{FileSystem, Path}\n",
    "import org.apache.hadoop.conf.Configuration\n",
    "\n",
    "val conf = new Configuration()\n",
    "val fs = FileSystem.get(conf)\n",
    "\n",
    "val parquetPaths = fs\n",
    "  .globStatus(\n",
    "    new Path(\"/workspaces/*/data/taxi/yellow_tripdata_2023-*.parquet\")\n",
    "  )\n",
    "  .map(_.getPath().toString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ecf4920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mlog\u001b[39m: \u001b[32mLogger\u001b[39m = org.apache.log4j.Logger@12074140"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "implicit val log = Logger.getLogger(this.getClass())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e51e943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.storage.StorageLevel\u001b[39m"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.storage.StorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fbc0d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/04 20:17:43 WARN cmd8$Helper: Casting 'VendorID' from LongType to IntegerType\n",
      "26/01/04 20:17:43 WARN cmd8$Helper: Casting 'tpep_pickup_datetime' from TimestampNTZType to TimestampType\n",
      "26/01/04 20:17:43 WARN cmd8$Helper: Casting 'tpep_dropoff_datetime' from TimestampNTZType to TimestampType\n",
      "26/01/04 20:17:43 WARN cmd8$Helper: Casting 'RatecodeID' from DoubleType to LongType\n",
      "26/01/04 20:17:43 WARN cmd8$Helper: Casting 'PULocationID' from LongType to IntegerType\n",
      "26/01/04 20:17:43 WARN cmd8$Helper: Casting 'DOLocationID' from LongType to IntegerType\n",
      "26/01/04 20:17:43 WARN cmd8$Helper: Casting 'tpep_pickup_datetime' from TimestampNTZType to TimestampType\n",
      "26/01/04 20:17:43 WARN cmd8$Helper: Casting 'tpep_dropoff_datetime' from TimestampNTZType to TimestampType\n",
      "26/01/04 20:17:43 WARN cmd8$Helper: Casting 'passenger_count' from LongType to DoubleType\n",
      "26/01/04 20:17:43 WARN cmd8$Helper: Casting 'tpep_pickup_datetime' from TimestampNTZType to TimestampType\n",
      "26/01/04 20:17:43 WARN cmd8$Helper: Casting 'tpep_dropoff_datetime' from TimestampNTZType to TimestampType\n",
      "26/01/04 20:17:43 WARN cmd8$Helper: Casting 'passenger_count' from LongType to DoubleType\n",
      "26/01/04 20:17:43 WARN cmd8$Helper: Casting 'tpep_pickup_datetime' from TimestampNTZType to TimestampType\n",
      "26/01/04 20:17:43 WARN cmd8$Helper: Casting 'tpep_dropoff_datetime' from TimestampNTZType to TimestampType\n",
      "26/01/04 20:17:43 WARN cmd8$Helper: Casting 'passenger_count' from LongType to DoubleType\n",
      "26/01/04 20:17:43 WARN cmd8$Helper: Casting 'tpep_pickup_datetime' from TimestampNTZType to TimestampType\n",
      "26/01/04 20:17:43 WARN cmd8$Helper: Casting 'tpep_dropoff_datetime' from TimestampNTZType to TimestampType\n",
      "26/01/04 20:17:43 WARN cmd8$Helper: Casting 'passenger_count' from LongType to DoubleType\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mparquets\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mDataFrame\u001b[39m] = \u001b[33mArray\u001b[39m(\n",
       "  [VendorID: bigint, tpep_pickup_datetime: timestamp_ntz ... 17 more fields],\n",
       "  [VendorID: int, tpep_pickup_datetime: timestamp_ntz ... 17 more fields],\n",
       "  [VendorID: int, tpep_pickup_datetime: timestamp_ntz ... 17 more fields],\n",
       "  [VendorID: int, tpep_pickup_datetime: timestamp_ntz ... 17 more fields],\n",
       "  [VendorID: int, tpep_pickup_datetime: timestamp_ntz ... 17 more fields]\n",
       ")\n",
       "\u001b[36mnormParquets\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mDataFrame\u001b[39m] = \u001b[33mArray\u001b[39m(\n",
       "  [VendorID: int, tpep_pickup_datetime: timestamp ... 17 more fields],\n",
       "  [VendorID: int, tpep_pickup_datetime: timestamp ... 17 more fields],\n",
       "  [VendorID: int, tpep_pickup_datetime: timestamp ... 17 more fields],\n",
       "  [VendorID: int, tpep_pickup_datetime: timestamp ... 17 more fields],\n",
       "  [VendorID: int, tpep_pickup_datetime: timestamp ... 17 more fields]\n",
       ")\n",
       "\u001b[36mtaxi\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mDataset\u001b[39m[\u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mRow\u001b[39m] = [VendorID: int, tpep_pickup_datetime: timestamp ... 17 more fields]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val parquets = parquetPaths.map(p => spark.read.parquet(p))\n",
    "val normParquets = parquets.map(SchemaEnforcer.enforce)\n",
    "val taxi = normParquets.reduce((a, b) => a.unionByName(b)).cache() // .persist(StorageLevel.MEMORY_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9819f2dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mpickupsPerHour\u001b[39m: \u001b[32mDataFrame\u001b[39m = [hour: int, pickups_per_hour: bigint]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pickupsPerHour = taxi\n",
    "  .select(\n",
    "    $\"tpep_pickup_datetime\"\n",
    "  )\n",
    "  .filter(F.dayofweek($\"tpep_pickup_datetime\").between(2, 6))\n",
    "  .withColumn(\"hour\", F.hour($\"tpep_pickup_datetime\"))\n",
    "  .groupBy($\"hour\")\n",
    "  .agg(F.count($\"tpep_pickup_datetime\").alias(\"pickups_per_hour\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce348b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "// pickupsPerHour.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebb3c283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mpercentile\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m774615L\u001b[39m"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val percentile = pickupsPerHour.select(F.percentile_approx(\n",
    "  $\"pickups_per_hour\",\n",
    "  F.lit(0.9),\n",
    "  F.lit(10000)\n",
    ")).as[Long].head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "859b024d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mrushHours\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mDataset\u001b[39m[\u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mRow\u001b[39m] = [hour: int, pickups_per_hour: bigint]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rushHours = pickupsPerHour.filter(\n",
    "  $\"pickups_per_hour\" > percentile\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e682c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+\n",
      "|hour|pickups_per_hour|\n",
      "+----+----------------+\n",
      "|  17|          821064|\n",
      "|  18|          876583|\n",
      "+----+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rushHours.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e885acec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mrushHoursArr\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mInt\u001b[39m] = \u001b[33mArray\u001b[39m(\u001b[32m17\u001b[39m, \u001b[32m18\u001b[39m)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rushHoursArr = rushHours.select($\"hour\").as[Int].collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19abf1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: double (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- airport_fee: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "taxi.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f78f98cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mduration_minutes\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mColumn\u001b[39m = ((unix_timestamp(tpep_dropoff_datetime, yyyy-MM-dd HH:mm:ss) - unix_timestamp(tpep_pickup_datetime, yyyy-MM-dd HH:mm:ss)) / 60)\n",
       "\u001b[36mrushHourStatistics\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mDataset\u001b[39m[\u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mRow\u001b[39m] = [PULocationID: int, DOLocationID: int ... 4 more fields]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val duration_minutes = (F.unix_timestamp($\"tpep_dropoff_datetime\") - F.unix_timestamp($\"tpep_pickup_datetime\")) / 60\n",
    "val rushHourStatistics = taxi\n",
    "  .select($\"*\", F.hour($\"tpep_pickup_datetime\").alias(\"hour\"))\n",
    "  .filter($\"hour\".isInCollection(rushHoursArr))\n",
    "  .groupBy($\"PULocationID\", $\"DOLocationID\")\n",
    "  .agg(\n",
    "    F.count($\"*\").alias(\"count\"),\n",
    "    F.avg(duration_minutes).alias(\"avg_duration_minutes\"),\n",
    "    F.avg($\"trip_distance\" / (duration_minutes / 60)).alias(\"avg_mph\"),\n",
    "    F.avg($\"fare_amount\" / $\"trip_distance\").alias(\"avg_fare_per_mile\")\n",
    "  ).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7857049b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mzones\u001b[39m: \u001b[32mDataFrame\u001b[39m = [LocationID: int, Borough: string ... 1 more field]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val zones = spark.read\n",
    "  .option(\"header\", \"true\")\n",
    "  .csv(\"/workspaces/*/data/taxi/taxi_zone_lookup.csv\")\n",
    "  .select(\n",
    "    $\"LocationID\".cast(T.IntegerType),\n",
    "    $\"Borough\",\n",
    "    $\"Zone\"\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e8d43e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- LocationID: integer (nullable = true)\n",
      " |-- Borough: string (nullable = true)\n",
      " |-- Zone: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zones.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27582f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      " |-- avg_duration_minutes: double (nullable = true)\n",
      " |-- avg_mph: double (nullable = true)\n",
      " |-- avg_fare_per_mile: double (nullable = true)\n",
      " |-- PU_Borough: string (nullable = true)\n",
      " |-- PU_Zone: string (nullable = true)\n",
      " |-- DO_Borough: string (nullable = true)\n",
      " |-- DO_Zone: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mrushHourStatisticsWithZones\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mDataset\u001b[39m[\u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mRow\u001b[39m] = [DOLocationID: int, PULocationID: int ... 8 more fields]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rushHourStatisticsWithZones = rushHourStatistics\n",
    "  .join(\n",
    "    zones.select(\n",
    "      $\"LocationID\".alias(\"PULocationID\"),\n",
    "      $\"Borough\".alias(\"PU_Borough\"),\n",
    "      $\"Zone\".alias(\"PU_Zone\")\n",
    "    ),\n",
    "    Seq(\"PULocationID\"),\n",
    "    \"left\"\n",
    "  )\n",
    "  .join(\n",
    "    zones.select(\n",
    "      $\"LocationID\".alias(\"DOLocationID\"),\n",
    "      $\"Borough\".alias(\"DO_Borough\"),\n",
    "      $\"Zone\".alias(\"DO_Zone\")\n",
    "    ),\n",
    "    Seq(\"DOLocationID\"),\n",
    "    \"left\"\n",
    "  ).cache()\n",
    "rushHourStatisticsWithZones.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06cb05f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+-----+--------------------+------------------+------------------+----------+---------------------+----------+-------------------------+\n",
      "|DOLocationID|PULocationID|count|avg_duration_minutes|avg_mph           |avg_fare_per_mile |PU_Borough|PU_Zone              |DO_Borough|DO_Zone                  |\n",
      "+------------+------------+-----+--------------------+------------------+------------------+----------+---------------------+----------+-------------------------+\n",
      "|264         |264         |17558|17.32253958309602   |16.61390159724997 |18.59398844705124 |Unknown   |N/A                  |Unknown   |N/A                      |\n",
      "|236         |237         |16264|7.6514008444007215  |10.769124136880796|8.085515046210366 |Manhattan |Upper East Side South|Manhattan |Upper East Side North    |\n",
      "|237         |236         |15717|8.344477317554237   |14.122857957733673|9.045409279918841 |Manhattan |Upper East Side North|Manhattan |Upper East Side South    |\n",
      "|237         |237         |11409|6.699176089052501   |9.693920683163524 |19.94635600344402 |Manhattan |Upper East Side South|Manhattan |Upper East Side South    |\n",
      "|236         |236         |11337|6.148000646849556   |9.883876706877542 |16.03501665039553 |Manhattan |Upper East Side North|Manhattan |Upper East Side North    |\n",
      "|237         |161         |9498 |10.10413771320278   |7.446566136793261 |9.667665662737186 |Manhattan |Midtown Center       |Manhattan |Upper East Side South    |\n",
      "|236         |161         |8153 |14.35979802935524   |9.362714962839734 |7.18155679653979  |Manhattan |Midtown Center       |Manhattan |Upper East Side North    |\n",
      "|239         |142         |7449 |6.950566071508479   |9.951732624628903 |8.535854970032732 |Manhattan |Lincoln Square East  |Manhattan |Upper West Side South    |\n",
      "|142         |239         |7295 |7.458608636052094   |9.162447388732778 |9.412092941690377 |Manhattan |Upper West Side South|Manhattan |Lincoln Square East      |\n",
      "|238         |239         |6898 |6.211624142263461   |10.593874714430658|8.847223204737585 |Manhattan |Upper West Side South|Manhattan |Upper West Side North    |\n",
      "|142         |237         |6227 |11.776181681922811  |7.835221935906385 |8.696488310393518 |Manhattan |Upper East Side South|Manhattan |Lincoln Square East      |\n",
      "|161         |237         |6213 |11.234985782499063  |6.93314117941621  |10.145330500149369|Manhattan |Upper East Side South|Manhattan |Midtown Center           |\n",
      "|237         |162         |6167 |8.506345602940382   |8.280135459160077 |9.488962724894016 |Manhattan |Midtown East         |Manhattan |Upper East Side South    |\n",
      "|142         |236         |6084 |14.182612864343648  |9.75463091659     |6.991728444563688 |Manhattan |Upper East Side North|Manhattan |Lincoln Square East      |\n",
      "|234         |161         |6018 |13.19347789963443   |7.609186412966056 |8.404094522711596 |Manhattan |Midtown Center       |Manhattan |Union Sq                 |\n",
      "|239         |238         |5793 |5.388952183669946   |456.9747978854457 |10.531439588099518|Manhattan |Upper West Side North|Manhattan |Upper West Side South    |\n",
      "|141         |236         |5785 |8.86463843272832    |8.541219550174258 |8.748679613535542 |Manhattan |Upper East Side North|Manhattan |Lenox Hill West          |\n",
      "|239         |236         |5785 |10.935263612791708  |10.098436164248172|7.448093277892073 |Manhattan |Upper East Side North|Manhattan |Upper West Side South    |\n",
      "|230         |132         |5705 |56.98111305872039   |20.053585720488158|3.805095800319143 |Queens    |JFK Airport          |Manhattan |Times Sq/Theatre District|\n",
      "|238         |142         |5697 |9.688953250248666   |11.419662486612895|6.9011123018093565|Manhattan |Lincoln Square East  |Manhattan |Upper West Side North    |\n",
      "+------------+------------+-----+--------------------+------------------+------------------+----------+---------------------+----------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rushHourStatisticsWithZones.orderBy($\"count\".desc).limit(20).show(truncate=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3630be1-1ba1-4cb4-bce0-8abb460d9cfb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "# Task 2: Demand Volatility\n",
    "\n",
    "**Goal:** Rank pickup zones by how much their trip volume fluctuates hourly.\n",
    "\n",
    "* **Step 1:** Count trips per `(Zone, Hour)` using `reduceByKey`.\n",
    "* **Step 2:** Regroup by `Zone` only.\n",
    "* **Step 3:** Calculate **Variance** or **StdDev** of the hourly counts.\n",
    "* **RDD Ops:** `map` (to hour)  `reduceByKey`  `aggregateByKey` (to compute stats)  `sortBy`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be217033-7038-4b25-8d28-33172948a32c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Comparison Goal\n",
    "\n",
    "Implement both using **RDDs** (manual logic) vs. **DataFrames** (SQL/Optimization) to compare:\n",
    "\n",
    "1. **Code Complexity:** Lines of code.\n",
    "2. **Performance:** Execution time and shuffle size in Spark UI.\n",
    "\n",
    "**Would you like the specific mathematical formulas for the RDD variance calculation?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a2b3a47-555b-494c-ba99-4cf1793285e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36ma\u001b[39m: \u001b[32mList\u001b[39m[\u001b[32mInt\u001b[39m] = \u001b[33mList\u001b[39m(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val a = List(1,2,3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
