{
    "name": "Scala",
    "build": {
        "dockerfile": "Dockerfile"
    },
    // NOTE: we make hostname consistent to not have to resolve random generated 
    //       container hostname when connecting to the standalone Spark cluster
    "runArgs": [
        "--name",
        "${localWorkspaceFolderBasename}",
        "--hostname",
        "${localWorkspaceFolderBasename}"
    ],
    "mounts": [
        // Persist /var/spark-events between container restarts
        {
            "source": "${localWorkspaceFolderBasename}-spark-events",
            "target": "/var/spark-events",
            "type": "volume"
        },
        // persist coursier cache between container restarts
        {
            "source": "${localWorkspaceFolderBasename}-coursier-cache",
            "target": "/home/ubuntu/.cache/coursier",
            "type": "volume"
        }
    ],
    // Use root to run postCreateCommand chown, then switch to 'ubuntu' user
    "containerUser": "root",
    "remoteUser": "ubuntu",
    "postCreateCommand": {
        // Link the custom spark-defaults.conf into SPARK_HOME/conf
        "linkSparkConfig": "ln -s /workspaces/${localWorkspaceFolderBasename}/spark-defaults.conf ${containerEnv:SPARK_HOME}/conf/spark-defaults.conf",
        // Mounting a volume sets root as owner, so we need to chown it to ubuntu
        // NOTE: running as root user (see containerUser), so no sudo needed
        "chownSparkEvents": "chown -R ubuntu:ubuntu /var/spark-events",
        // NOTE: this command was supposed to be executed by ubuntu user, but we only have root to run commands at container creation, so we run it as root and chown it
        "linkJupyterConfig": "mkdir -p ~/.jupyter && ln -s /workspaces/${localWorkspaceFolderBasename}/jupyter_server_config.json /home/ubuntu/.jupyter/jupyter_server_config.json && chown -R ubuntu:ubuntu /home/ubuntu/.jupyter"
    },
    // runs as remoteUser 'ubuntu'
    "postStartCommand": {
        "startSpark": "${containerWorkspaceFolder}/scripts/start-spark.sh",
        // NOTE: we redirect to /dev/null to prevent nohup from streaming the output to ./nohup.out
        "startJupyter": "nohup bash -c 'jupyter lab --no-browser --ip=127.0.0.1 --port=8888 > .jupyter.log 2>&1 &' > /dev/null"
    },
    // NOTE: More ports will be forwarded by VSCode automatically, but those are the main ones used by scripts in ./scripts/
    "forwardPorts": [
        8080,  // Spark Master Web UI
        8081,  // Spark Worker Web UI
        18080, // Spark History Server Web UI
        4040,  // Spark Application Web UI
        8888   // Jupyter Notebook
    ],
    // NOTE: JAVA_TOOL_OPTIONS is used to avoid almond kernel issues with Spark 3.3.X & Java 17+
    //       See: https://stackoverflow.com/questions/73465937/apache-spark-3-3-0-breaks-on-java-17-with-cannot-access-class-sun-nio-ch-direct
    "remoteEnv": {
        "PATH": "${containerEnv:SPARK_HOME}/bin:${containerEnv:SPARK_HOME}/sbin:${containerEnv:PATH}",
        "JAVA_TOOL_OPTIONS": "--add-exports=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED"
    },
    "containerEnv": {
        "JAVA_TOOL_OPTIONS": "--add-exports=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED"
    },
    "customizations": {
        "vscode": {
            "settings": {
                "terminal.integrated.defaultProfile.linux": "bash"
            },
            "extensions": [
                "streetsidesoftware.code-spell-checker",
                "berublan.vscode-log-viewer",
                "mechatroner.rainbow-csv",
                "mads-hartmann.bash-ide-vscode",
                "scala-lang.scala",
                "scalameta.metals",
                "ms-toolsai.jupyter",
                "christian-kohler.path-intellisense"
            ]
        }
    }
}