# GeoTrellis / Scala devcontainer
# - Spark 3.3.x + Scala 2.13 (via manual install)
# - Java 17 (latest supported for Spark 3.3.x)
# NOTE: eclipse-temurin is how you get openjdk now
FROM eclipse-temurin:17

# Make all RUN commands use 'set -eux' by default with pipefail for better command pipelines
SHELL ["/bin/bash", "-eux", "-o", "pipefail", "-c"]

# Spark binary version
# NOTE: We use Spark 3.3.X as this is the latest version compatible with GeoTrellis
# NOTE: Scala version is 2.13.8
ARG SPARK_VERSION=3.3.4
ARG HADOOP_VERSION=3
ARG SCALA_BINARY_VERSION=2.13

# WARNING: Generally, before installing packages, verify they will be compatible with the scala version bundled with Spark
#       VIEW AVAILABLE VERSIONS BY VISITING MVN FOR EACH PACKAGE!
# coursier Scala version
ARG CS_VERSION=2.1.24
ARG SBT_VERSION=1.11.7
ARG SCALA_CLI_VERSION=1.6.1
ARG AMMONITE_VERSION=3.0.4
# NOTE: This warning was resolved by detecting scala version from spark jars at build time, but is still relevant for ALMOND_VERSION
# WARNING: SCAlA VERSION MUST MATCH THE SCALA VERSION OF SPARK!
#          MISMATCHING VERSIONS (EVEN 2.13.17 VS 2.13.8) WILL CAUSE CRYPTIC ERRORS (WHICH CAN TAKE DAYS TO DEBUG!)
#          LIKE `Can only call getServletHandlers on a running MetricsSystem`
#          WHEN CONNECTING TO SPARK FROM THE NOTEBOOK(!),
#          WITH THEM ONLY BEING SURFACED IN `$SPARK_HOME/logs/spark--org.apache.spark.deploy.master.Master-1-spark-localhost.out`
# See scala 2.13.8 compatible almond versions here: https://mvnrepository.com/artifact/sh.almond/scala-kernel_2.13.8
ARG ALMOND_VERSION=0.13.14
ARG SCALAFMT_VERSION=3.10.2

# Jupyterlab configuration
ARG PYTHON_VERSION=3.12

# spark user configuration
# NOTE: eclipse-temurin:17 comes with non root ubuntu user, so this is technically not necessary
# NOTE: UID and GID 1000 is preferred by vscode for workspace mount permissions resolution
ARG USERNAME=ubuntu
ARG USER_UID=1000
ARG USER_GID=$USER_UID

ENV SPARK_HOME=/opt/spark
ENV PATH="/home/${USERNAME}/.local/share/coursier/bin:${PATH}"

# NOTE: set -eux
#       -e: exit on error
#       -u: fail on undefined vars
#       -x: print executed commands	

# Base tools
USER root
# Install curl for downloads, unzip for awscli, ca-certificates for https, gnupg for apt over https
RUN apt-get update \
    && apt-get install -y --no-install-recommends \
    curl \
    unzip \
    ca-certificates \
    gnupg \
    && rm -rf /var/lib/apt/lists/*

# Install Spark 3.3.x (Scala 2.13)
RUN curl -fsSL "https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}-scala${SCALA_BINARY_VERSION}.tgz" \
    -o /tmp/spark.tgz \
    && mkdir -p /opt \
    && tar -xzf /tmp/spark.tgz -C /opt \
    && mv "/opt/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}-scala${SCALA_BINARY_VERSION}" "${SPARK_HOME}" \
    && rm /tmp/spark.tgz

# coursier (Scala artifact fetcher)
RUN curl -fsSL "https://github.com/coursier/coursier/releases/download/v${CS_VERSION}/cs-x86_64-pc-linux.gz" \
    -o /tmp/cs.gz; \
    gunzip /tmp/cs.gz; \
    install -m 0755 /tmp/cs /usr/local/bin/cs; \
    rm -f /tmp/cs

# spark user & devcontainer workspace dir
# See: https://code.visualstudio.com/remote/advancedcontainers/add-nonroot-user#_creating-a-nonroot-user
# NOTE: ignores if group id already exists; attempts to create a new user if ${USERNAME} with ${USER_UID} does not exist - fail if unsuccessful
RUN getent group "${USER_GID}" || groupadd --gid "${USER_GID}" "${USERNAME}" \
    && ( id "${USERNAME}" && [ "$(id -u "${USERNAME}")" = "${USER_UID}" ] \
    || useradd --uid "${USER_UID}" --gid "${USER_GID}" -m -s /bin/bash "${USERNAME}" ) \
    && mkdir -p /workspaces \
    && ( chown -R "${USER_UID}:${USER_GID}" "/home/${USERNAME}" /workspaces || true )

# Derive Scala patch version from Spark distribution
# (AND AVOID MISMATCH BETWEEN INSTALLED SCALA AND SPARK SCALA VERSIONS!)
RUN SCALA_VERSION="$(basename /opt/spark/jars/scala-library-*.jar .jar | cut -d- -f3)"; \
    echo "SCALA_VERSION=$SCALA_VERSION" | tee /etc/spark-scala.env

# Install Scala and dev-tools
# NOTE: We `cs install` and not `cs setup` because we want to be explicit about package versions
USER ${USERNAME}
RUN . /etc/spark-scala.env; \
    mkdir -p ~/.local/share/coursier/bin \
    && cs install "scala:${SCALA_VERSION}" \
    "scalac:${SCALA_VERSION}" \
    "scala-cli:${SCALA_CLI_VERSION}" \
    "sbt:${SBT_VERSION}" \
    "sbtn:${SBT_VERSION}" \
    "ammonite:${AMMONITE_VERSION}" \
    "scalafmt:${SCALAFMT_VERSION}" \
    --install-dir ~/.local/share/coursier/bin
# Install almond kernel
RUN . /etc/spark-scala.env; \
    cs launch --use-bootstrap almond:${ALMOND_VERSION} --scala ${SCALA_VERSION} -- --install

# Install GDAL to verify Geotrellis results
USER root
RUN apt-get update \
    && apt-get install -y --no-install-recommends \
    gdal-bin \
    && rm -rf /var/lib/apt/lists/*

# Install AWS CLI v2
USER root
RUN curl -fsSL "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o /tmp/awscliv2.zip \
    && unzip -q /tmp/awscliv2.zip -d /tmp \
    && /tmp/aws/install \
    && rm -rf /tmp/aws /tmp/awscliv2.zip

# Additional utilities
# NOTE: openjdk-17-source is installed for metals Java classes goto-definition support
USER root
RUN apt-get update \
    && apt-get install -y --no-install-recommends \
    git \
    bash \
    less \
    vim \
    && rm -rf /var/lib/apt/lists/*

# Install Python3.12
USER root
RUN apt-get update \
    && apt-get install -y --no-install-recommends \
    python3.12 \
    && rm -rf /var/lib/apt/lists/*
# NOTE: eclipse-temurin:17 doesn't come with available apt installable python3-pip
RUN curl -fsSL https://bootstrap.pypa.io/get-pip.py -o /tmp/get-pip.py \
    && python3.12 /tmp/get-pip.py --break-system-packages --root-user-action=ignore \
    && rm /tmp/get-pip.py
# Install jupyterlab
RUN pip3 install --no-cache-dir --break-system-packages --root-user-action=ignore 'jupyterlab>=4.1.0,<5.0.0a0' jupyterlab-lsp

USER ${USERNAME}
RUN cs install metals --install-dir ~/.local/share/coursier/bin

# NOTE: creating /var/spark-events before mounting (in devcontainer.json) ensures proper permissions
USER root
RUN mkdir -p /var/spark-events \
    && chown ${USERNAME}:${USERNAME} /var/spark-events

USER ${USERNAME}